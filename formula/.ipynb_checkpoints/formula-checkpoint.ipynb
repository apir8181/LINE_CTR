{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINE 1st order\n",
    "\n",
    "Node i neighbour distribution probability\n",
    "$$ p(v_j | v_i) = \\frac{ exp(x_j^T x_i) }{ \\sum_k exp(x_k^T x_i) } $$\n",
    "\n",
    "Node i Empircal distribution\n",
    "$$ \\hat{p}(v_j | v_i) = \\frac{ I[(i,j) \\in \\mathbb{E}] }{ \\sum_k I[(i,k) \\in \\mathbb{E}] } $$\n",
    "\n",
    "KL Divergence\n",
    "$$ \\begin{align}\n",
    "\\mathbb{D}_{KL} &= \\sum_i \\sum_{(i, j) \\in \\mathbb{E}} \\hat{p}(v_j | v_i) log\\frac{ \\hat{p}(v_j | v_i) }{ p(v_j | v_i) } \\\\\n",
    "&= \\sum_i \\sum_{(i, j) \\in \\mathbb{E}} (-\\frac{1}{d_i}log{d_i} - \\frac{1}{d_i}log{ p(v_j | v_i) }) \\\\\n",
    "&\\approx \\sum_i \\sum_{(i, j) \\in \\mathbb{E}} -\\frac{1}{d_i}(log\\sigma(x_i^Tx_j) + \\sum_klog\\sigma(-x_i^Tx_k)) - \\sum_i \\sum_{(i, j) \\in \\mathbb{E}}\\frac{1}{d_i}log{d_i}\n",
    "\\end{align} $$\n",
    "where $d_i$ is number of edges node i contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label embedding\n",
    "\n",
    "Node i on AD l likelihood\n",
    "$$ \\mathbb{L}_{il} = c_{il} log\\sigma(x_i^T y_l) + e_{il} log\\sigma(-x_i^T y_l) $$\n",
    "where $c_{il}$ is AD interest empirical probability, $e_{il}$ is AD exposed but not interest empirical probability.\n",
    "\n",
    "Total click likelihood\n",
    "$$ \\mathbb{L} = \\sum_{i} \\sum_{l} c_{il} log\\sigma(x_i^T y_l) + e_{il} log\\sigma(-x_i^T y_l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointly Optimization\n",
    "\n",
    "$$ \\begin{align}\n",
    "min \\mathbb{O} &= \\mathbb{D}_{KL} - \\lambda \\mathbb{L} \\\\\n",
    "&= \\sum_i \\sum_{(i, j) \\in \\mathbb{E}} -\\frac{1}{d_i}(log\\sigma(x_i^Tx_j) + \\sum_klog\\sigma(-x_i^Tx_k)) - \\sum_i \\sum_{(i, j) \\in \\mathbb{E}}\\frac{1}{d_i}log{d_i} - \\lambda \\sum_{i} \\sum_{l} (c_{il} log\\sigma(x_i^T y_l) + e_{il} log\\sigma(-x_i^T y_l)) \\\\\n",
    "&\\sim \\sum_i \\sum_{(i, j) \\in \\mathbb{E}} -\\frac{1}{d_i}(log\\sigma(x_i^Tx_j) + \\sum_klog\\sigma(-x_i^Tx_k))  - \\lambda \\sum_{i} \\sum_{l} (c_{il} log\\sigma(x_i^T y_l) + e_{il} log\\sigma(-x_i^T y_l)) \\\\\n",
    "&\\sim \\sum_i [\\sum_{(i, j) \\in \\mathbb{E}} -\\frac{1}{d_i}(log\\sigma(x_i^Tx_j) + \\sum_klog\\sigma(-x_i^Tx_k))  - \\lambda \\sum_{l} (c_{il} log\\sigma(x_i^T y_l) + e_{il} log\\sigma(-x_i^T y_l))]\n",
    "\\end{align}$$\n",
    "\n",
    "To apply stochastic gradient descent, we sample a node, an edge belong to this node and an AD exposed to this node.\n",
    "$$ \\mathbb{O}_{ijl} = -(log\\sigma(x_i^Tx_j) + \\sum_klog\\sigma(-x_i^Tx_k)) - \\frac{\\lambda}{N_l} (c_{il} log\\sigma(x_i^T y_l) + e_{il} log\\sigma(-x_i^T y_l)) $$\n",
    "$$ \\frac{ \\partial \\mathbb{O}_{ijl} }{ \\partial x_i } = -(\\sigma(-x_i^Tx_j)x_j - \\sum_k \\sigma(x_i^Tx_k)x_k) - \\frac{\\lambda}{N_l} (c_{il} \\sigma(-x_i^T y_l) - e_{il} \\sigma(x_i^T y_l))y_l $$\n",
    "$$ \\frac{ \\partial \\mathbb{O}_{ijl} }{ \\partial x_j } = -\\sigma(-x_i^Tx_j)x_i $$\n",
    "$$ \\frac{ \\partial \\mathbb{O}_{ijl} }{ \\partial x_k } = \\sigma(x_i^Tx_k)x_i $$\n",
    "$$ \\frac{ \\partial \\mathbb{O}_{ijl} }{ \\partial y_l } = -\\frac{\\lambda}{N_l} (c_{il} \\sigma(-x_i^T y_l) - e_{il} \\sigma(x_i^T y_l))x_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
